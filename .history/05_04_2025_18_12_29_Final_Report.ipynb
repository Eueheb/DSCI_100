{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fd60c0-74b8-4468-a663-68c9ae10b558",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f03587-9d35-4331-bd84-96f846bab3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac17a13-1e66-415d-ad53-7b659a23a267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3ce2b2-2811-4244-81bd-e8f268fc7fbe",
   "metadata": {},
   "source": [
    "## Methods & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bcc21-54eb-4fdc-9d68-1c5850399ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidymodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c64ab-fd45-41b0-8009-d209b36451b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data <- read_csv(\"data/players.csv\")\n",
    "players_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17544f-2b02-4996-8473-1c66724798a6",
   "metadata": {},
   "source": [
    "Our analysis starts by importing the dataset directly from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21165d-0ce4-4d1f-a551-fa9f1feed2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_tidy <- players_data |>\n",
    "  select (experience, subscribe, played_hours, Age) |>\n",
    "  filter (Age != 17) |>\n",
    "  mutate (experience = as_factor(experience),\n",
    "        subscribe = as_factor(subscribe))\n",
    "head(players_tidy)\n",
    "players_tidy <- players_tidy %>%\n",
    "  mutate(subscribe = fct_relevel(subscribe, \"TRUE\")) #This simply makes TRUE in subscribe column the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201232d7-a3bc-4737-a2e9-23688600970f",
   "metadata": {},
   "source": [
    "After loading, we clean the dataset to include only the relevant variables: experience, subscribe, played_hours, and Age. These variables are chosen because they relate closely to user behavior and potential factors influencing subscription decisions. Players aged exactly 17 are removed, because they form an unrepresentative group that could introduce noise. Converting experience and subscribe to categorical types ensures that these variables are treated correctly during modeling, especially since subscribe is the outcome we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b2b24-84eb-4727-b85c-bfe03e363d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 <- players_tidy |>\n",
    "  ggplot(aes(x = played_hours, fill = subscribe)) +\n",
    "  geom_histogram(binwidth = 5, alpha = 0.7, position = \"identity\") +\n",
    "  labs(title = \"Figure 1: Distribution of Played Hours by Subscription\",\n",
    "       x = \"Played Hours\", y = \"Count\") +\n",
    "  theme_minimal()\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2692585-8cdd-48ab-86a2-16bd50c78230",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 <- players_tidy |>\n",
    "  ggplot(aes(x = experience, y = played_hours, fill = experience, alpha = 0.1)) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Figure 2: Played Hours by Experience Level\",\n",
    "       x = \"Experience Level\", y = \"Played Hours\") +\n",
    "  theme_minimal()\n",
    "p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b82b3-3506-432d-8129-e46b2aadc4ed",
   "metadata": {},
   "source": [
    "The next step involves visual exploration of the data. A histogram is created to compare how playtime is distributed across subscription statuses. This visualization provides an intuitive way to assess whether more engaged users tend to subscribe and whether there is a pattern that could inform predictions. The bin width is chosen to balance granularity and clarity. A second plot—a boxplot—illustrates how playtime differs across experience levels. This helps us explore whether experience is related to the amount of time spent playing, which might be another indicator of subscription behavior. Both visualizations serve to uncover possible relationships among the variables and the target, guiding feature selection and model expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a55db1-a9b3-4f5d-8b1a-e7e81b7c155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "data_split <- initial_split(players_tidy, prop = 0.8, strata = subscribe)\n",
    "train_data <- training(data_split)\n",
    "test_data <- testing(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3cb11-8bc2-4652-9f99-d194a115ccb2",
   "metadata": {},
   "source": [
    "The dataset is then split into training and testing sets with an 80/20 ratio. This ensures enough data is available for training while reserving a portion for final model evaluation. Stratifying by the outcome variable ensures that both subsets have similar class distributions, which is critical for classification tasks to avoid biased or misleading results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70c8b0-729b-4a51-842b-823ae21805d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_recipe <- recipe(subscribe ~ played_hours + Age + experience, data = train_data) |>\n",
    "  step_normalize(all_numeric_predictors()) |>\n",
    "  step_dummy(all_nominal_predictors())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ba699-1bd6-4c60-920c-007eb36a1f1a",
   "metadata": {},
   "source": [
    "We then create a preprocessing recipe . Normalizing numeric variables is essential because the model we’re using is sensitive to scale—larger-scale features could dominate smaller ones and distort the distance calculations. Dummy encoding is applied to the experience variable, allowing the categorical levels to be represented in a way the model can interpret and process effectively. This preprocessing ensures the model receives clean, consistent input without hidden biases from scaling or encoding mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7270e7-a407-440d-a8ff-9f8c228dae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_spec <- nearest_neighbor(neighbors = tune(), weight_func = \"rectangular\") |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70bb72-c8a8-42b0-9e60-b3c583a80fc7",
   "metadata": {},
   "source": [
    "Next, we specify the k-nearest neighbors classification model, allowing the number of neighbors (k) to be tuned. This setup is chosen because k-NN is intuitive and well-suited for problems where similarity in feature space corresponds to similar outcomes. Setting the number of neighbors as a parameter to be tuned because model performance is highly dependent on this value—too small, and the model overfits; too large, and it may underfit or smooth out important distinctions. Therefore we want to test out different k to determine which works the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48fe05-d6e9-41cd-8df2-ded6a4fa6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_workflow <- workflow() |>\n",
    "  add_model(knn_spec) |>\n",
    "  add_recipe(knn_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa257f38-fe4e-44b8-87b2-44536c9645a2",
   "metadata": {},
   "source": [
    "We combine the preprocessing and model definition into a single workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903a662-7c89-4f63-a308-581a57dec6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "folds <- vfold_cv(train_data, v = 5, strata = subscribe)\n",
    "\n",
    "k_vals <- tibble(neighbors = seq(from = 1, to = 20, by = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02646889-6ade-4713-bdff-04ce9bb95716",
   "metadata": {},
   "source": [
    "To find the optimal number of neighbors, we conduct 10-fold cross-validation. This method offers a robust way to evaluate the model’s performance across various partitions of the training data. It mitigates the effects of random variation from any single train-test split and produces more stable estimates of model performance. We evaluate a range of values for k, from 1 to 20, to identify the configuration that results in the highest classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d1441-ac82-4bf2-809d-de9c16dd5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tune_results <- knn_workflow |>\n",
    "    tune_grid(resamples = folds, grid = k_vals) |>\n",
    "    collect_metrics() |>\n",
    "    filter(.metric == \"accuracy\")\n",
    "knn_tune_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b44e29-7efc-47a4-8f79-a50e9f3ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k <- knn_tune_results |>\n",
    "  arrange(-mean) |>\n",
    "    slice(1) |>\n",
    "    pull(neighbors)\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e52723-7b50-428e-b734-854234e68d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot <- knn_tune_results |>\n",
    "  ggplot(aes(x = neighbors, y = mean)) +\n",
    "  geom_line(color = \"blue\") +\n",
    "  geom_point(color = \"red\") +\n",
    "  geom_vline(xintercept = best_k, linetype = \"dashed\", color = \"darkgreen\") +\n",
    "  labs(title = \"Figure 3: Accuracy vs Number of Neighbors (k)\",\n",
    "       x = \"Number of Neighbors (k)\",\n",
    "       y = \"Cross-Validated Accuracy\") +\n",
    "  theme_minimal()\n",
    "knn_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afcab7c-21b4-4789-9969-8ec4cd4c14ba",
   "metadata": {},
   "source": [
    "The model is tuned across the grid of k values, and the one yielding the best accuracy across all folds is selected, which turns out to be 15. This process helps identify a balance between model flexibility and stability. Visualizing the tuning results as a line plot makes it easy to interpret how accuracy changes with the number of neighbors. The plot reveals whether performance improves or plateaus and helps justify the selection of the best k. As shown in the figure, the best k is 15 because it has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dc05b-7807-4f56-9faa-ccdf52a0a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn_spec <- nearest_neighbor(neighbors = best_k, weight_func = \"rectangular\") |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "final_knn_workflow <- workflow() |>\n",
    "  add_model(final_knn_spec) |>\n",
    "  add_recipe(knn_recipe)\n",
    "\n",
    "final_fit <- fit(final_knn_workflow, data = train_data)\n",
    "\n",
    "test_accuracy <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  metrics(truth = subscribe, estimate = .pred_class) |>\n",
    "  filter(.metric == \"accuracy\")\n",
    "\n",
    "test_precision <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  precision(truth = subscribe, estimate = .pred_class)\n",
    "\n",
    "test_recall <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  recall(truth = subscribe, estimate = .pred_class)\n",
    "\n",
    "test_accuracy\n",
    "\n",
    "test_precision\n",
    "\n",
    "test_recall\n",
    "\n",
    "confusion_matrix <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  conf_mat(truth = subscribe, estimate = .pred_class)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec51ba-02bb-4a54-8e12-abaa5956a672",
   "metadata": {},
   "source": [
    "With the optimal value chosen, the workflow is finalized and retrained on the full training dataset. This ensures that the model benefits from all available training data before making final predictions. The model is then applied to the test set, providing an unbiased evaluation of its real-world performance. The results include  overall accuracy, precision, recall and a confusion matrix, offering insight into how well the model predicts each class and where it might make mistakes. This final step validates the effectiveness of the model and helps us understand its strengths and limitations in a practical context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

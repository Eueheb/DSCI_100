{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fd60c0-74b8-4468-a663-68c9ae10b558",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671a6c5-12cd-44dd-8385-893b759329bb",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2bc6f-a2d4-4a17-b15d-21bc4a1be538",
   "metadata": {},
   "source": [
    "In the ever-evolving world of online games, understanding player behavior is critical for developing engaging experiences and effective communication strategies. One emerging area of interest lies in examining the ways in which in-game behavior and demographic information might predict a player’s engagement outside of gameplay—such as subscribing to a game-related newsletter. This connection between in-game actions and out-of-game engagement can offer valuable insights for both game developers and researchers interested in player psychology and behavior modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baeb03c-0f66-4d0f-a9da-ae2bdee36efb",
   "metadata": {},
   "source": [
    "The specific question we aim to answer in this project is: *Can playing duration and age be used to predict whether a player subscribes to a game-related newsletter, and does their in-game experience affect this outcome?* This question sits within a broader exploration of which player characteristics and behaviors are most predictive of newsletter subscription, and how these predictors may differ across various types of players. The motivation behind this question stems from a desire to bridge the gap between in-game analytics and marketing efforts, with the hope of improving player communication through data-driven strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e4b64-6163-41ff-a20f-18bcda8fc8b3",
   "metadata": {},
   "source": [
    "To address this question, we will analyze data from the players.csv file provided by the Minecraft research server project. This dataset contains 7 variables and 196 observations, representing player experience, subscription status, hashed email, total playtime, name, gender, and age. The hashed emails and names are irrelevant as they do not contribute to answering the question. A potential issue in the gender variable is the presence of \"Prefer not to say,\" which may complicate gender-based analyses. Additionally, the age column has an unusually high number of 17-year-old entries due to a default setting, which may distort the actual age distribution. The most relevant variables for predicting newsletter subscription are playing duration and player experience, as they contain fewer or no significant outliers, making them ideal for identifying patterns in subscription behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b03fb-6128-434b-90d3-e7205a646974",
   "metadata": {},
   "source": [
    "To answer our research question, we will select the experience, subscribe, age, and played_hours columns from the dataset. We will convert the experience column into a factor variable to allow for clear visualization, either by coloring different player types or by generating separate plots for each experience level. Due to the abundance of default values set to 17 in the age column, we will consider filtering these observations to avoid skewing the results. After cleaning the data, we will standardize the age and played_hours variables and use them in a K-Nearest Neighbors (K-NN) classification model to explore whether these features can predict newsletter subscription.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ce2b2-2811-4244-81bd-e8f268fc7fbe",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bcc21-54eb-4fdc-9d68-1c5850399ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidymodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c64ab-fd45-41b0-8009-d209b36451b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data <- read_csv(\"data/players.csv\")\n",
    "head(players_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17544f-2b02-4996-8473-1c66724798a6",
   "metadata": {},
   "source": [
    "Our analysis starts by importing the dataset directly from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21165d-0ce4-4d1f-a551-fa9f1feed2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_tidy <- players_data |>\n",
    "  select (experience, subscribe, played_hours, Age) |>\n",
    "  filter (Age != 17) |>\n",
    "  mutate (experience = as_factor(experience),\n",
    "        subscribe = as_factor(subscribe))\n",
    "head(players_tidy)\n",
    "players_tidy <- players_tidy %>%\n",
    "  mutate(subscribe = fct_relevel(subscribe, \"TRUE\")) #This simply makes TRUE in subscribe column the positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201232d7-a3bc-4737-a2e9-23688600970f",
   "metadata": {},
   "source": [
    "After loading, we clean the dataset to include only the relevant variables: experience, subscribe, played_hours, and Age. These variables are chosen because they relate closely to user behavior and potential factors influencing subscription decisions. Players aged exactly 17 are removed, because they form an unrepresentative group that could introduce noise. Converting experience and subscribe to categorical types ensures that these variables are treated correctly during modeling, especially since subscribe is the outcome we aim to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b2b24-84eb-4727-b85c-bfe03e363d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 <- players_tidy |>\n",
    "  ggplot(aes(x = played_hours, fill = subscribe)) +\n",
    "  geom_histogram(binwidth = 5, alpha = 0.7, position = \"identity\") +\n",
    "  labs(title = \"Figure 1: Distribution of Played Hours by Subscription\",\n",
    "       x = \"Played Hours\", y = \"Count\") +\n",
    "  theme_minimal()\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2692585-8cdd-48ab-86a2-16bd50c78230",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 <- players_tidy |>\n",
    "  ggplot(aes(x = experience, y = played_hours, fill = experience, alpha = 0.1)) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Figure 2: Played Hours by Experience Level\",\n",
    "       x = \"Experience Level\", y = \"Played Hours\") +\n",
    "  theme_minimal()\n",
    "p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b82b3-3506-432d-8129-e46b2aadc4ed",
   "metadata": {},
   "source": [
    "The next step involves visual exploration of the data. A histogram is created to compare how playtime is distributed across subscription statuses. This visualization provides an intuitive way to assess whether more engaged users tend to subscribe and whether there is a pattern that could inform predictions. The bin width is chosen to balance granularity and clarity. A second plot—a boxplot—illustrates how playtime differs across experience levels. This helps us explore whether experience is related to the amount of time spent playing, which might be another indicator of subscription behavior. Both visualizations serve to uncover possible relationships among the variables and the target, guiding feature selection and model expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a55db1-a9b3-4f5d-8b1a-e7e81b7c155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "data_split <- initial_split(players_tidy, prop = 0.8, strata = subscribe)\n",
    "train_data <- training(data_split)\n",
    "test_data <- testing(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc3cb11-8bc2-4652-9f99-d194a115ccb2",
   "metadata": {},
   "source": [
    "The dataset is then split into training and testing sets with an 80/20 ratio. This ensures enough data is available for training while reserving a portion for final model evaluation. Stratifying by the outcome variable ensures that both subsets have similar class distributions, which is critical for classification tasks to avoid biased or misleading results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70c8b0-729b-4a51-842b-823ae21805d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_recipe <- recipe(subscribe ~ played_hours + Age + experience, data = train_data) |>\n",
    "  step_normalize(all_numeric_predictors()) |>\n",
    "  step_dummy(all_nominal_predictors())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ba699-1bd6-4c60-920c-007eb36a1f1a",
   "metadata": {},
   "source": [
    "We then create a preprocessing recipe . Normalizing numeric variables is essential because the model we’re using is sensitive to scale—larger-scale features could dominate smaller ones and distort the distance calculations. Dummy encoding is applied to the experience variable, allowing the categorical levels to be represented in a way the model can interpret and process effectively. This preprocessing ensures the model receives clean, consistent input without hidden biases from scaling or encoding mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7270e7-a407-440d-a8ff-9f8c228dae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_spec <- nearest_neighbor(neighbors = tune(), weight_func = \"rectangular\") |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70bb72-c8a8-42b0-9e60-b3c583a80fc7",
   "metadata": {},
   "source": [
    "Next, we specify the k-nearest neighbors classification model, allowing the number of neighbors (k) to be tuned. This setup is chosen because k-NN is intuitive and well-suited for problems where similarity in feature space corresponds to similar outcomes. Setting the number of neighbors as a parameter to be tuned because model performance is highly dependent on this value—too small, and the model overfits; too large, and it may underfit or smooth out important distinctions. Therefore we want to test out different k to determine which works the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48fe05-d6e9-41cd-8df2-ded6a4fa6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_workflow <- workflow() |>\n",
    "  add_model(knn_spec) |>\n",
    "  add_recipe(knn_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa257f38-fe4e-44b8-87b2-44536c9645a2",
   "metadata": {},
   "source": [
    "We combine the preprocessing and model definition into a single workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903a662-7c89-4f63-a308-581a57dec6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "folds <- vfold_cv(train_data, v = 10, strata = subscribe)\n",
    "\n",
    "k_vals <- tibble(neighbors = seq(from = 1, to = 20, by = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02646889-6ade-4713-bdff-04ce9bb95716",
   "metadata": {},
   "source": [
    "To find the optimal number of neighbors, we conduct 10-fold cross-validation. This method offers a robust way to evaluate the model’s performance across various partitions of the training data. It mitigates the effects of random variation from any single train-test split and produces more stable estimates of model performance. We evaluate a range of values for k, from 1 to 20, to identify the configuration that results in the highest classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d1441-ac82-4bf2-809d-de9c16dd5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tune_results <- knn_workflow |>\n",
    "    tune_grid(resamples = folds, grid = k_vals) |>\n",
    "    collect_metrics() |>\n",
    "    filter(.metric == \"accuracy\")\n",
    "knn_tune_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b44e29-7efc-47a4-8f79-a50e9f3ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k <- knn_tune_results |>\n",
    "  arrange(-mean) |>\n",
    "    slice(1) |>\n",
    "    pull(neighbors)\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e52723-7b50-428e-b734-854234e68d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot <- knn_tune_results |>\n",
    "  ggplot(aes(x = neighbors, y = mean)) +\n",
    "  geom_line(color = \"blue\") +\n",
    "  geom_point(color = \"red\") +\n",
    "  geom_vline(xintercept = best_k, linetype = \"dashed\", color = \"darkgreen\") +\n",
    "  labs(title = \"Figure 3: Accuracy vs Number of Neighbors (k)\",\n",
    "       x = \"Number of Neighbors (k)\",\n",
    "       y = \"Cross-Validated Accuracy\") +\n",
    "  theme_minimal()\n",
    "knn_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afcab7c-21b4-4789-9969-8ec4cd4c14ba",
   "metadata": {},
   "source": [
    "The model is tuned across the grid of k values, and the one yielding the best accuracy across all folds is selected, which turns out to be 15. This process helps identify a balance between model flexibility and stability. Visualizing the tuning results as a line plot makes it easy to interpret how accuracy changes with the number of neighbors. The plot reveals whether performance improves or plateaus and helps justify the selection of the best k. As shown in the figure, the best k is 15 because it has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dc05b-7807-4f56-9faa-ccdf52a0a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn_spec <- nearest_neighbor(neighbors = best_k, weight_func = \"rectangular\") |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "final_knn_workflow <- workflow() |>\n",
    "  add_model(final_knn_spec) |>\n",
    "  add_recipe(knn_recipe)\n",
    "\n",
    "final_fit <- fit(final_knn_workflow, data = train_data)\n",
    "\n",
    "test_accuracy <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  metrics(truth = subscribe, estimate = .pred_class) |>\n",
    "  filter(.metric == \"accuracy\")\n",
    "\n",
    "test_precision <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  precision(truth = subscribe, estimate = .pred_class)\n",
    "\n",
    "test_recall <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  recall(truth = subscribe, estimate = .pred_class)\n",
    "\n",
    "test_accuracy\n",
    "\n",
    "test_precision\n",
    "\n",
    "test_recall\n",
    "\n",
    "confusion_matrix <- predict(final_fit, test_data) |>\n",
    "  bind_cols(test_data) |>\n",
    "  conf_mat(truth = subscribe, estimate = .pred_class)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec51ba-02bb-4a54-8e12-abaa5956a672",
   "metadata": {},
   "source": [
    "With the optimal value chosen, the workflow is finalized and retrained on the full training dataset. This ensures that the model benefits from all available training data before making final predictions. The model is then applied to the test set, providing an unbiased evaluation of its real-world performance. The results include  overall accuracy, precision, recall and a confusion matrix, offering insight into how well the model predicts each class and where it might make mistakes. This final step validates the effectiveness of the model and helps us understand its strengths and limitations in a practical context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6dc6b-b1a9-4425-92bb-6e9a3e2915b1",
   "metadata": {},
   "source": [
    "## Discusion & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1b814-3f75-4516-81b4-e2e9eb4a7956",
   "metadata": {},
   "source": [
    "This project explored whether a player's age and playing duration can predict their likelihood of subscribing to a game-related newsletter, and whether a player’s experience level influences this outcome. Using a K-Nearest Neighbors (K-NN) classification model and cross-validation from players.csv, we found that the optimal number of neighbors was 13. The final model achieved an accuracy of 75% and a precision of 76.2%. These results indicate that the model is particularly strong at identifying actual subscribers, as seen with high recall of 94.1%. Although, it does make some incorrect predictions, as reflected by a slightly lower precision. Importantly, the inclusion of experience level as a categorical variable improved the model’s ability to distinguish patterns among different types of players. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848808-41dc-454b-ae9a-7de1a310a45e",
   "metadata": {},
   "source": [
    "These findings partially align with our expectations. We predicted that longer playtime and older age would be associated with higher likelihood of subscription, and that experience level might influence this relationship. The model’s high recall supports this hypothesis. However, the lower precision suggests that subscription behavior may also depend on other factors not included in the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df268d-a148-4d8d-86c9-329a503fe973",
   "metadata": {},
   "source": [
    "The ability to predict out-of-game engagement based on in-game behavior could be valuable for game developers and marketers. If certain patterns in playtime and experience levels correlate with higher subscription rates, targeted communication strategies— such as in-game prompts or personalized offers—could be developed to encourage newsletter signups. Furthermore, this approach illustrates how behavioral analytics can enable more tailored and effective player outreach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b488b-59e5-44e9-bf46-cb19b4efc57d",
   "metadata": {},
   "source": [
    "Lastly, this study opens the door for further researches and questions, such as “Would adding more behavioral variables (eg. session frequency, type of gameplay) improve prediction?” or “How does subscription behavior vary across different experience levels when analyzed separately?”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
